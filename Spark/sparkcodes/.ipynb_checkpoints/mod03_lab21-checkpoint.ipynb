{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 執行 row 2、4即可以跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkSessionv\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn \n",
    "from pyspark.sql.types import StringType,DoubleType,IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for standalone(lightweight) pyspark\n",
    "# create spar session object\n",
    "spark=SparkSession.builder.appName('data_processing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySparkShell'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.appName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv Dataset \n",
    "df=spark.read.csv('data/sample_data.csv',inferSchema=True,header=True)\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ratings', 'age', 'experience', 'family', 'mobile']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns of dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataset\n",
    "df.count(),len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ratings: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: double (nullable = true)\n",
      " |-- family: integer (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print dataframe schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+\n",
      "|ratings|age|experience|family| mobile|\n",
      "+-------+---+----------+------+-------+\n",
      "|      3| 32|       9.0|     3|   Vivo|\n",
      "|      3| 27|      13.0|     3|  Apple|\n",
      "|      4| 22|       2.5|     0|Samsung|\n",
      "|      4| 37|      16.5|     4|  Apple|\n",
      "|      5| 27|       9.0|     1|     MI|\n",
      "|      4| 27|       9.0|     0|   Oppo|\n",
      "|      5| 37|      23.0|     5|   Vivo|\n",
      "|      5| 37|      23.0|     5|Samsung|\n",
      "|      3| 22|       2.5|     0|  Apple|\n",
      "|      3| 27|       6.0|     0|     MI|\n",
      "|      2| 27|       6.0|     2|   Oppo|\n",
      "|      5| 27|       6.0|     2|Samsung|\n",
      "|      3| 37|      16.5|     5|  Apple|\n",
      "|      5| 27|       6.0|     0|     MI|\n",
      "|      4| 22|       6.0|     1|   Oppo|\n",
      "|      4| 37|       9.0|     2|Samsung|\n",
      "|      4| 27|       6.0|     1|  Apple|\n",
      "|      1| 37|      23.0|     5|     MI|\n",
      "|      2| 42|      23.0|     2|   Oppo|\n",
      "|      4| 37|       6.0|     0|   Vivo|\n",
      "+-------+---+----------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display fisrt few rows of dataframe\n",
    "df.show()\n",
    "#df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['mobile], [unresolvedalias('mobile, None), count(1) AS count#65L]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mobile: string, count: bigint\n",
      "Aggregate [mobile#20], [mobile#20, count(1) AS count#65L]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [mobile#20], [mobile#20, count(1) AS count#65L]\n",
      "+- Project [mobile#20]\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[mobile#20], functions=[count(1)], output=[mobile#20, count#65L])\n",
      "+- Exchange hashpartitioning(mobile#20, 200), ENSURE_REQUIREMENTS, [id=#94]\n",
      "   +- *(1) HashAggregate(keys=[mobile#20], functions=[partial_count(1)], output=[mobile#20, count#69L])\n",
      "      +- FileScan csv [mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('mobile').count().explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| mobile|count|\n",
      "+-------+-----+\n",
      "|     MI|    8|\n",
      "|   Oppo|    7|\n",
      "|Samsung|    6|\n",
      "|   Vivo|    5|\n",
      "|  Apple|    7|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by one column\n",
    "df.groupBy('mobile').count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['count DESC NULLS LAST], true\n",
      "+- Aggregate [mobile#20], [mobile#20, count(1) AS count#102L]\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mobile: string, count: bigint\n",
      "Sort [count#102L DESC NULLS LAST], true\n",
      "+- Aggregate [mobile#20], [mobile#20, count(1) AS count#102L]\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [count#102L DESC NULLS LAST], true\n",
      "+- Aggregate [mobile#20], [mobile#20, count(1) AS count#102L]\n",
      "   +- Project [mobile#20]\n",
      "      +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [count#102L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(count#102L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#143]\n",
      "   +- *(2) HashAggregate(keys=[mobile#20], functions=[count(1)], output=[mobile#20, count#102L])\n",
      "      +- Exchange hashpartitioning(mobile#20, 200), ENSURE_REQUIREMENTS, [id=#139]\n",
      "         +- *(1) HashAggregate(keys=[mobile#20], functions=[partial_count(1)], output=[mobile#20, count#106L])\n",
      "            +- FileScan csv [mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort value counts\n",
    "df.groupBy('mobile').count().orderBy('count',ascending=False).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| mobile|count|\n",
      "+-------+-----+\n",
      "|     MI|    8|\n",
      "|   Oppo|    7|\n",
      "|  Apple|    7|\n",
      "|Samsung|    6|\n",
      "|   Vivo|    5|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort value counts\n",
    "df.groupBy('mobile').count().orderBy('count',ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "| mobile|      avg(ratings)|          avg(age)|   avg(experience)|       avg(family)|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|     MI|               3.5|            30.125|           10.1875|             1.375|\n",
      "|   Oppo| 2.857142857142857|28.428571428571427|10.357142857142858|1.4285714285714286|\n",
      "|Samsung| 4.166666666666667|28.666666666666668| 8.666666666666666|1.8333333333333333|\n",
      "|   Vivo|               4.2|              36.0|              11.4|               1.8|\n",
      "|  Apple|3.4285714285714284|30.571428571428573|              11.0|2.7142857142857144|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate statistical measures\n",
    "df.groupBy('mobile').mean().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+---------------+-----------+\n",
      "| mobile|sum(ratings)|sum(age)|sum(experience)|sum(family)|\n",
      "+-------+------------+--------+---------------+-----------+\n",
      "|     MI|          28|     241|           81.5|         11|\n",
      "|   Oppo|          20|     199|           72.5|         10|\n",
      "|Samsung|          25|     172|           52.0|         11|\n",
      "|   Vivo|          21|     180|           57.0|          9|\n",
      "|  Apple|          24|     214|           77.0|         19|\n",
      "+-------+------------+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate statistical measures\n",
    "df.groupBy('mobile').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['mobile], [unresolvedalias('mobile, None), max(ratings#16) AS max(ratings)#79, max(age#17) AS max(age)#80, max(experience#18) AS max(experience)#81, max(family#19) AS max(family)#82]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mobile: string, max(ratings): int, max(age): int, max(experience): double, max(family): int\n",
      "Aggregate [mobile#20], [mobile#20, max(ratings#16) AS max(ratings)#79, max(age#17) AS max(age)#80, max(experience#18) AS max(experience)#81, max(family#19) AS max(family)#82]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [mobile#20], [mobile#20, max(ratings#16) AS max(ratings)#79, max(age#17) AS max(age)#80, max(experience#18) AS max(experience)#81, max(family#19) AS max(family)#82]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[mobile#20], functions=[max(ratings#16), max(age#17), max(experience#18), max(family#19)], output=[mobile#20, max(ratings)#79, max(age)#80, max(experience)#81, max(family)#82])\n",
      "+- Exchange hashpartitioning(mobile#20, 200), ENSURE_REQUIREMENTS, [id=#113]\n",
      "   +- *(1) HashAggregate(keys=[mobile#20], functions=[partial_max(ratings#16), partial_max(age#17), partial_max(experience#18), partial_max(family#19)], output=[mobile#20, max#92, max#93, max#94, max#95])\n",
      "      +- FileScan csv [ratings#16,age#17,experience#18,family#19,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate statistical measures\n",
    "df.groupBy('mobile').max().explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+---------------+-----------+\n",
      "| mobile|max(ratings)|max(age)|max(experience)|max(family)|\n",
      "+-------+------------+--------+---------------+-----------+\n",
      "|     MI|           5|      42|           23.0|          5|\n",
      "|   Oppo|           4|      42|           23.0|          2|\n",
      "|Samsung|           5|      37|           23.0|          5|\n",
      "|   Vivo|           5|      37|           23.0|          5|\n",
      "|  Apple|           4|      37|           16.5|          5|\n",
      "+-------+------------+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate statistical measures\n",
    "df.groupBy('mobile').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+---------------+-----------+\n",
      "| mobile|min(ratings)|min(age)|min(experience)|min(family)|\n",
      "+-------+------------+--------+---------------+-----------+\n",
      "|     MI|           1|      27|            2.5|          0|\n",
      "|   Oppo|           2|      22|            6.0|          0|\n",
      "|Samsung|           2|      22|            2.5|          0|\n",
      "|   Vivo|           3|      32|            6.0|          0|\n",
      "|  Apple|           3|      22|            2.5|          0|\n",
      "+-------+------------+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate statistical measures\n",
    "df.groupBy('mobile').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| mobile|count|\n",
      "+-------+-----+\n",
      "|     MI|    8|\n",
      "|   Oppo|    7|\n",
      "|Samsung|    6|\n",
      "|   Vivo|    5|\n",
      "|  Apple|    7|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql\n",
    "spark.sql('''select mobile, count(*) as count from dfTable\n",
    "        group by mobile''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------+\n",
      "| mobile|min(experience)|min(age)|\n",
      "+-------+---------------+--------+\n",
      "|     MI|            2.5|      27|\n",
      "|   Oppo|            6.0|      22|\n",
      "|Samsung|            2.5|      22|\n",
      "|   Vivo|            6.0|      32|\n",
      "|  Apple|            2.5|      22|\n",
      "+-------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql\n",
    "spark.sql('''select mobile, min(experience), min(age) from dfTable\n",
    "        group by mobile''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------+\n",
      "| mobile|min(experience)|min(age)|\n",
      "+-------+---------------+--------+\n",
      "|     MI|            2.5|      27|\n",
      "|   Oppo|            6.0|      22|\n",
      "|Samsung|            2.5|      22|\n",
      "|   Vivo|            6.0|      32|\n",
      "|  Apple|            2.5|      22|\n",
      "+-------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('mobile').min('experience','age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "| mobile|sum(experience)|\n",
      "+-------+---------------+\n",
      "|     MI|           81.5|\n",
      "|   Oppo|           72.5|\n",
      "|Samsung|           52.0|\n",
      "|   Vivo|           57.0|\n",
      "|  Apple|           77.0|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation\n",
    "df.groupBy('mobile').agg({'experience':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "| mobile|sum(experience)|\n",
      "+-------+---------------+\n",
      "|     MI|           81.5|\n",
      "|   Oppo|           72.5|\n",
      "|Samsung|           52.0|\n",
      "|   Vivo|           57.0|\n",
      "|  Apple|           77.0|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation\n",
    "df.groupBy('mobile').agg({'experience':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['mobile], [unresolvedalias('mobile, None), min(experience#18) AS min(experience)#173, min(age#17) AS min(age)#174]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mobile: string, min(experience): double, min(age): int\n",
      "Aggregate [mobile#20], [mobile#20, min(experience#18) AS min(experience)#173, min(age#17) AS min(age)#174]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [mobile#20], [mobile#20, min(experience#18) AS min(experience)#173, min(age#17) AS min(age)#174]\n",
      "+- Project [age#17, experience#18, mobile#20]\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[mobile#20], functions=[min(experience#18), min(age#17)], output=[mobile#20, min(experience)#173, min(age)#174])\n",
      "+- Exchange hashpartitioning(mobile#20, 200), ENSURE_REQUIREMENTS, [id=#237]\n",
      "   +- *(1) HashAggregate(keys=[mobile#20], functions=[partial_min(experience#18), partial_min(age#17)], output=[mobile#20, min#180, min#181])\n",
      "      +- FileScan csv [age#17,experience#18,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:int,experience:double,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('mobile').min('experience','age').explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['mobile], ['mobile, unresolvedalias('min('experience), None), unresolvedalias('min('age), None)]\n",
      "+- 'UnresolvedRelation [dfTable], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mobile: string, min(experience): double, min(age): int\n",
      "Aggregate [mobile#20], [mobile#20, min(experience#18) AS min(experience)#184, min(age#17) AS min(age)#185]\n",
      "+- SubqueryAlias dftable\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [mobile#20], [mobile#20, min(experience#18) AS min(experience)#184, min(age#17) AS min(age)#185]\n",
      "+- Project [age#17, experience#18, mobile#20]\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[mobile#20], functions=[min(experience#18), min(age#17)], output=[mobile#20, min(experience)#184, min(age)#185])\n",
      "+- Exchange hashpartitioning(mobile#20, 200), ENSURE_REQUIREMENTS, [id=#256]\n",
      "   +- *(1) HashAggregate(keys=[mobile#20], functions=[partial_min(experience#18), partial_min(age#17)], output=[mobile#20, min#191, min#192])\n",
      "      +- FileScan csv [age#17,experience#18,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:int,experience:double,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql\n",
    "spark.sql('''select mobile, min(experience), min(age) from dfTable\n",
    "        group by mobile''').explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['mobile], [unresolvedalias('mobile, None), 'min(experience#18) AS min(experience)#198, 'min(age#17) AS min(age)#199]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mobile: string, min(experience): double, min(age): int\n",
      "Aggregate [mobile#20], [mobile#20, min(experience#18) AS min(experience)#198, min(age#17) AS min(age)#199]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [mobile#20], [mobile#20, min(experience#18) AS min(experience)#198, min(age#17) AS min(age)#199]\n",
      "+- Project [age#17, experience#18, mobile#20]\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[mobile#20], functions=[min(experience#18), min(age#17)], output=[mobile#20, min(experience)#198, min(age)#199])\n",
      "+- Exchange hashpartitioning(mobile#20, 200), ENSURE_REQUIREMENTS, [id=#275]\n",
      "   +- *(1) HashAggregate(keys=[mobile#20], functions=[partial_min(experience#18), partial_min(age#17)], output=[mobile#20, min#207, min#208])\n",
      "      +- FileScan csv [age#17,experience#18,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:int,experience:double,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation\n",
    "df.groupBy('mobile').agg({'experience':'min','age':'min'}).explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv Dataset\n",
    "rtdf=spark.read.csv('data/online_retail_dataset.csv',inferSchema=True,header=True)\n",
    "rtdf.createOrReplaceTempView(\"rtTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID',\n",
       " 'Country']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns of dataframe\n",
    "rtdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(541909, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataset\n",
    "rtdf.count(),len(rtdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print dataframe schema\n",
    "rtdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display fisrt few rows of dataframe\n",
    "rtdf.show()\n",
    "#rtdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "|536365   |22752    |SET 7 BABUSHKA NESTING BOXES       |2       |12/1/2010 8:26|7.65     |17850     |United Kingdom|\n",
      "|536365   |21730    |GLASS STAR FROSTED T-LIGHT HOLDER  |6       |12/1/2010 8:26|4.25     |17850     |United Kingdom|\n",
      "|536366   |22633    |HAND WARMER UNION JACK             |6       |12/1/2010 8:28|1.85     |17850     |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT          |6       |12/1/2010 8:28|1.85     |17850     |United Kingdom|\n",
      "|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT      |32      |12/1/2010 8:34|1.69     |13047     |United Kingdom|\n",
      "|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM          |6       |12/1/2010 8:34|2.1      |13047     |United Kingdom|\n",
      "|536367   |22748    |POPPY'S PLAYHOUSE KITCHEN          |6       |12/1/2010 8:34|2.1      |13047     |United Kingdom|\n",
      "|536367   |22749    |FELTCRAFT PRINCESS CHARLOTTE DOLL  |8       |12/1/2010 8:34|3.75     |13047     |United Kingdom|\n",
      "|536367   |22310    |IVORY KNITTED MUG COSY             |6       |12/1/2010 8:34|1.65     |13047     |United Kingdom|\n",
      "|536367   |84969    |BOX OF 6 ASSORTED COLOUR TEASPOONS |6       |12/1/2010 8:34|4.25     |13047     |United Kingdom|\n",
      "|536367   |22623    |BOX OF VINTAGE JIGSAW BLOCKS       |3       |12/1/2010 8:34|4.95     |13047     |United Kingdom|\n",
      "|536367   |22622    |BOX OF VINTAGE ALPHABET BLOCKS     |2       |12/1/2010 8:34|9.95     |13047     |United Kingdom|\n",
      "|536367   |21754    |HOME BUILDING BLOCK WORD           |3       |12/1/2010 8:34|5.95     |13047     |United Kingdom|\n",
      "|536367   |21755    |LOVE BUILDING BLOCK WORD           |3       |12/1/2010 8:34|5.95     |13047     |United Kingdom|\n",
      "|536367   |21777    |RECIPE BOX WITH METAL HEART        |4       |12/1/2010 8:34|7.95     |13047     |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rtdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# counting\n",
    "rtdf.select(fn.count('StockCode')).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'count(StockCode)'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting\n",
    "fn.count('StockCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distinct count\n",
    "rtdf.select(fn.countDistinct('StockCode')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [min('Quantity) AS min(Quantity)#285, max('Quantity) AS max(Quantity)#286]\n",
      "+- Relation[InvoiceNo#225,StockCode#226,Description#227,Quantity#228,InvoiceDate#229,UnitPrice#230,CustomerID#231,Country#232] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "min(Quantity): int, max(Quantity): int\n",
      "Aggregate [min(Quantity#228) AS min(Quantity)#285, max(Quantity#228) AS max(Quantity)#286]\n",
      "+- Relation[InvoiceNo#225,StockCode#226,Description#227,Quantity#228,InvoiceDate#229,UnitPrice#230,CustomerID#231,Country#232] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [min(Quantity#228) AS min(Quantity)#285, max(Quantity#228) AS max(Quantity)#286]\n",
      "+- Project [Quantity#228]\n",
      "   +- Relation[InvoiceNo#225,StockCode#226,Description#227,Quantity#228,InvoiceDate#229,UnitPrice#230,CustomerID#231,Country#232] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[min(Quantity#228), max(Quantity#228)], output=[min(Quantity)#285, max(Quantity)#286])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#328]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_min(Quantity#228), partial_max(Quantity#228)], output=[min#291, max#292])\n",
      "      +- FileScan csv [Quantity#228] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/online_retail_dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Quantity:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get minimun and maximun\n",
    "rtdf.select(fn.min(\"Quantity\"), fn.max(\"Quantity\")).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get minimun and maximun\n",
    "rtdf.select(fn.min(\"Quantity\"), fn.max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|47559.30364660923| 47559.39140929892|  218.08095663447835|   218.08115785023455|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variance and Standard Deviation\n",
    "rtdf.select(fn.var_pop('Quantity'), fn.var_samp('Quantity'),\n",
    "        fn.stddev_pop('Quantity'), fn.stddev_samp('Quantity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use sql\n",
    "spark.sql('''select count(StockCode) from rtTable''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [corr('InvoiceNo, 'Quantity) AS corr(InvoiceNo, Quantity)#336, covar_samp('InvoiceNo, 'Quantity) AS covar_samp(InvoiceNo, Quantity)#345, covar_pop('InvoiceNo, 'Quantity) AS covar_pop(InvoiceNo, Quantity)#354]\n",
      "+- Relation[InvoiceNo#225,StockCode#226,Description#227,Quantity#228,InvoiceDate#229,UnitPrice#230,CustomerID#231,Country#232] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "corr(InvoiceNo, Quantity): double, covar_samp(InvoiceNo, Quantity): double, covar_pop(InvoiceNo, Quantity): double\n",
      "Aggregate [corr(cast(InvoiceNo#225 as double), cast(Quantity#228 as double)) AS corr(InvoiceNo, Quantity)#336, covar_samp(cast(InvoiceNo#225 as double), cast(Quantity#228 as double)) AS covar_samp(InvoiceNo, Quantity)#345, covar_pop(cast(InvoiceNo#225 as double), cast(Quantity#228 as double)) AS covar_pop(InvoiceNo, Quantity)#354]\n",
      "+- Relation[InvoiceNo#225,StockCode#226,Description#227,Quantity#228,InvoiceDate#229,UnitPrice#230,CustomerID#231,Country#232] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [corr(cast(InvoiceNo#225 as double), cast(Quantity#228 as double)) AS corr(InvoiceNo, Quantity)#336, covar_samp(cast(InvoiceNo#225 as double), cast(Quantity#228 as double)) AS covar_samp(InvoiceNo, Quantity)#345, covar_pop(cast(InvoiceNo#225 as double), cast(Quantity#228 as double)) AS covar_pop(InvoiceNo, Quantity)#354]\n",
      "+- Project [InvoiceNo#225, Quantity#228]\n",
      "   +- Relation[InvoiceNo#225,StockCode#226,Description#227,Quantity#228,InvoiceDate#229,UnitPrice#230,CustomerID#231,Country#232] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[corr(cast(InvoiceNo#225 as double), cast(Quantity#228 as double)), covar_samp(cast(InvoiceNo#225 as double), cast(Quantity#228 as double)), covar_pop(cast(InvoiceNo#225 as double), cast(Quantity#228 as double))], output=[corr(InvoiceNo, Quantity)#336, covar_samp(InvoiceNo, Quantity)#345, covar_pop(InvoiceNo, Quantity)#354])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#347]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_corr(cast(InvoiceNo#225 as double), cast(Quantity#228 as double)), partial_covar_samp(cast(InvoiceNo#225 as double), cast(Quantity#228 as double)), partial_covar_pop(cast(InvoiceNo#225 as double), cast(Quantity#228 as double))], output=[n#417, xAvg#418, yAvg#419, ck#420, xMk#421, yMk#422, n#427, xAvg#428, yAvg#429, ck#430, n#435, xAvg#436, yAvg#437, ck#438])\n",
      "      +- FileScan csv [InvoiceNo#225,Quantity#228] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/online_retail_dataset.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:string,Quantity:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Covariance and Correlation\n",
    "rtdf.select(fn.corr('InvoiceNo', 'Quantity'), fn.covar_samp('InvoiceNo', 'Quantity'),\n",
    "        fn.covar_pop('InvoiceNo', 'Quantity')).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085640497E-4|             1052.7280543915997|            1052.7260778754955|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Covariance and Correlation\n",
    "rtdf.select(fn.corr('InvoiceNo', 'Quantity'), fn.covar_samp('InvoiceNo', 'Quantity'),\n",
    "        fn.covar_pop('InvoiceNo', 'Quantity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "+---------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count with groupby\n",
    "rtdf.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|InvoiceNo|count(Quantity)|\n",
      "+---------+---------------+\n",
      "|   536596|              6|\n",
      "|   536938|             14|\n",
      "|   537252|              1|\n",
      "|   537691|             20|\n",
      "|   538041|              1|\n",
      "+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agg function\n",
    "rtdf.groupBy('InvoiceNo').agg({'Quantity':'count'}).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "+---------+----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agg function\n",
    "rtdf.groupBy('InvoiceNo').agg(fn.count('Quantity').alias('quan'),\n",
    "        fn.expr('count(Quantity)')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-------------+\n",
      "|InvoiceNo|max(UnitPrice)|min(Quantity)|\n",
      "+---------+--------------+-------------+\n",
      "|   536596|         19.95|            1|\n",
      "|   536938|         10.95|           20|\n",
      "|   537252|          0.85|           31|\n",
      "|   537691|          9.95|            2|\n",
      "|   538041|           0.0|           30|\n",
      "+---------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agg function\n",
    "rtdf.groupBy('InvoiceNo').agg({'Quantity':'min', 'UnitPrice':'max'}).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+\n",
      "|InvoiceNo|max(Quantity)|min(Quantity)|\n",
      "+---------+-------------+-------------+\n",
      "|   536596|            4|            1|\n",
      "|   536938|           72|           20|\n",
      "|   537252|           31|           31|\n",
      "|   537691|           24|            2|\n",
      "|   538041|           30|           30|\n",
      "+---------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agg function\n",
    "rtdf.groupBy('InvoiceNo').agg(fn.max('Quantity'),\n",
    "        fn.min('Quantity')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Python Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal function \n",
    "def price_range(brand):\n",
    "    if brand in ['Samsung','Apple']:\n",
    "        return 'High Price'\n",
    "    elif brand =='MI':\n",
    "        return 'Mid Price'\n",
    "    else:\n",
    "        return 'Low Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create udf using python function\n",
    "brand_udf=udf(price_range,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ratings: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: double (nullable = true)\n",
      " |-- family: integer (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+-----------+\n",
      "|ratings|age|experience|family|mobile |price_range|\n",
      "+-------+---+----------+------+-------+-----------+\n",
      "|3      |32 |9.0       |3     |Vivo   |Low Price  |\n",
      "|3      |27 |13.0      |3     |Apple  |High Price |\n",
      "|4      |22 |2.5       |0     |Samsung|High Price |\n",
      "|4      |37 |16.5      |4     |Apple  |High Price |\n",
      "|5      |27 |9.0       |1     |MI     |Mid Price  |\n",
      "|4      |27 |9.0       |0     |Oppo   |Low Price  |\n",
      "|5      |37 |23.0      |5     |Vivo   |Low Price  |\n",
      "|5      |37 |23.0      |5     |Samsung|High Price |\n",
      "|3      |22 |2.5       |0     |Apple  |High Price |\n",
      "|3      |27 |6.0       |0     |MI     |Mid Price  |\n",
      "+-------+---+----------+------+-------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply udf on dataframe\n",
    "df.withColumn('price_range',brand_udf(df['mobile'])).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using lambda function\n",
    "age_udf = udf(lambda age: \"young\" if age <= 30 else \"senior\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+---------+\n",
      "|ratings|age|experience|family| mobile|age_group|\n",
      "+-------+---+----------+------+-------+---------+\n",
      "|      3| 32|       9.0|     3|   Vivo|   senior|\n",
      "|      3| 27|      13.0|     3|  Apple|    young|\n",
      "|      4| 22|       2.5|     0|Samsung|    young|\n",
      "|      4| 37|      16.5|     4|  Apple|   senior|\n",
      "|      5| 27|       9.0|     1|     MI|    young|\n",
      "|      4| 27|       9.0|     0|   Oppo|    young|\n",
      "|      5| 37|      23.0|     5|   Vivo|   senior|\n",
      "|      5| 37|      23.0|     5|Samsung|   senior|\n",
      "|      3| 22|       2.5|     0|  Apple|    young|\n",
      "|      3| 27|       6.0|     0|     MI|    young|\n",
      "+-------+---+----------+------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply udf on dataframe\n",
    "df.withColumn(\"age_group\", age_udf(df.age)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas UDF (Spark 2.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType # PandasUDFType => Spark 對應到 Pandas 的資料型態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create python function\n",
    "# **想要用 pandas 的功能，即定義 pandas 的 function\n",
    "def remaining_yrs(age):\n",
    "    yrs_left=100-age\n",
    "    return yrs_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create udf using python function\n",
    "length_udf = pandas_udf(remaining_yrs, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+--------+\n",
      "|ratings|age|experience|family| mobile|yrs_left|\n",
      "+-------+---+----------+------+-------+--------+\n",
      "|      3| 32|       9.0|     3|   Vivo|      68|\n",
      "|      3| 27|      13.0|     3|  Apple|      73|\n",
      "|      4| 22|       2.5|     0|Samsung|      78|\n",
      "|      4| 37|      16.5|     4|  Apple|      63|\n",
      "|      5| 27|       9.0|     1|     MI|      73|\n",
      "+-------+---+----------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply pandas udf on dataframe\n",
    "df.withColumn('yrs_left', length_udf(df['age'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf using two columns \n",
    "def prod(rating,exp):\n",
    "    x=rating*exp\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create udf using python function\n",
    "# DoubleType() => 傳回執資料型態\n",
    "prod_udf = pandas_udf(prod, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+-------+\n",
      "|ratings|age|experience|family| mobile|product|\n",
      "+-------+---+----------+------+-------+-------+\n",
      "|      3| 32|       9.0|     3|   Vivo|   27.0|\n",
      "|      3| 27|      13.0|     3|  Apple|   39.0|\n",
      "|      4| 22|       2.5|     0|Samsung|   10.0|\n",
      "|      4| 37|      16.5|     4|  Apple|   66.0|\n",
      "|      5| 27|       9.0|     1|     MI|   45.0|\n",
      "+-------+---+----------+------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply pandas udf on multiple columns of dataframe\n",
    "df.withColumn(\"product\", prod_udf(df['ratings'],df['experience'])).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加入pandas 的 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接在 function 加入 pandas 元素，即可動用 pandas\n",
    "def prod_pandas(rating,exp):\n",
    "    x=rating*exp*np.pi\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_udf = pandas_udf(prod_pandas, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+------------------+\n",
      "|ratings|age|experience|family| mobile|           product|\n",
      "+-------+---+----------+------+-------+------------------+\n",
      "|      3| 32|       9.0|     3|   Vivo| 84.82300164692441|\n",
      "|      3| 27|      13.0|     3|  Apple|122.52211349000193|\n",
      "|      4| 22|       2.5|     0|Samsung| 31.41592653589793|\n",
      "|      4| 37|      16.5|     4|  Apple|207.34511513692635|\n",
      "|      5| 27|       9.0|     1|     MI| 141.3716694115407|\n",
      "+-------+---+----------+------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"product\", prod_udf(df['ratings'],df['experience'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use decorator\n",
    "@pandas_udf(IntegerType())\n",
    "def remaining_yrs2(age):\n",
    "    yrs_left=100-age\n",
    "    return yrs_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+--------+\n",
      "|ratings|age|experience|family| mobile|yrs_left|\n",
      "+-------+---+----------+------+-------+--------+\n",
      "|      3| 32|       9.0|     3|   Vivo|      68|\n",
      "|      3| 27|      13.0|     3|  Apple|      73|\n",
      "|      4| 22|       2.5|     0|Samsung|      78|\n",
      "|      4| 37|      16.5|     4|  Apple|      63|\n",
      "|      5| 27|       9.0|     1|     MI|      73|\n",
      "+-------+---+----------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply pandas udf on dataframe\n",
    "df.withColumn('yrs_left', remaining_yrs2(df['age'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use decorator\n",
    "@pandas_udf(DoubleType())\n",
    "def prod2(rating,exp):\n",
    "    x=rating*exp\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+-------+\n",
      "|ratings|age|experience|family| mobile|product|\n",
      "+-------+---+----------+------+-------+-------+\n",
      "|      3| 32|       9.0|     3|   Vivo|   27.0|\n",
      "|      3| 27|      13.0|     3|  Apple|   39.0|\n",
      "|      4| 22|       2.5|     0|Samsung|   10.0|\n",
      "|      4| 37|      16.5|     4|  Apple|   66.0|\n",
      "|      5| 27|       9.0|     1|     MI|   45.0|\n",
      "+-------+---+----------+------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply pandas udf on multiple columns of dataframe\n",
    "df.withColumn(\"product\", prod2(df['ratings'],df['experience'])).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas UDF (Spark 3.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas udf function\n",
    "@pandas_udf('int')\n",
    "def remaining_yrs3(age: pd.Series) -> pd.Series:\n",
    "    yrs_left=100-age\n",
    "    return yrs_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+--------+\n",
      "|ratings|age|experience|family| mobile|yrs_left|\n",
      "+-------+---+----------+------+-------+--------+\n",
      "|      3| 32|       9.0|     3|   Vivo|      68|\n",
      "|      3| 27|      13.0|     3|  Apple|      73|\n",
      "|      4| 22|       2.5|     0|Samsung|      78|\n",
      "|      4| 37|      16.5|     4|  Apple|      63|\n",
      "|      5| 27|       9.0|     1|     MI|      73|\n",
      "+-------+---+----------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply pandas udf on dataframe\n",
    "df.withColumn('yrs_left', remaining_yrs3(df['age'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas udf function\n",
    "@pandas_udf('double')\n",
    "def prod3(rating: pd.Series, exp: pd.Series) -> pd.Series:\n",
    "    x=rating*exp\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+-------+\n",
      "|ratings|age|experience|family| mobile|product|\n",
      "+-------+---+----------+------+-------+-------+\n",
      "|      3| 32|       9.0|     3|   Vivo|   27.0|\n",
      "|      3| 27|      13.0|     3|  Apple|   39.0|\n",
      "|      4| 22|       2.5|     0|Samsung|   10.0|\n",
      "|      4| 37|      16.5|     4|  Apple|   66.0|\n",
      "|      5| 27|       9.0|     1|     MI|   45.0|\n",
      "+-------+---+----------+------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply pandas udf on multiple columns of dataframe\n",
    "df.withColumn(\"product\", prod3(df['ratings'],df['experience'])).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+-------+\n",
      "|ratings|age|experience|family| mobile|product|\n",
      "+-------+---+----------+------+-------+-------+\n",
      "|      3| 32|       9.0|     3|   Vivo|   27.0|\n",
      "|      3| 27|      13.0|     3|  Apple|   39.0|\n",
      "|      4| 22|       2.5|     0|Samsung|   10.0|\n",
      "|      4| 37|      16.5|     4|  Apple|   66.0|\n",
      "|      5| 27|       9.0|     1|     MI|   45.0|\n",
      "+-------+---+----------+------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql \n",
    "df.selectExpr('*', 'experience*ratings as product').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, remaining_yrs(age#17) AS yrs_left#636]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string, yrs_left: int\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, remaining_yrs(age#17) AS yrs_left#636]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, pythonUDF0#643 AS yrs_left#636]\n",
      "+- ArrowEvalPython [remaining_yrs(age#17)], [pythonUDF0#643], 200\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ratings#16, age#17, experience#18, family#19, mobile#20, pythonUDF0#643 AS yrs_left#636]\n",
      "+- ArrowEvalPython [remaining_yrs(age#17)], [pythonUDF0#643], 200\n",
      "   +- FileScan csv [ratings#16,age#17,experience#18,family#19,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('yrs_left', length_udf(df['age'])).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, remaining_yrs2(age#17) AS yrs_left#627]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string, yrs_left: int\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, remaining_yrs2(age#17) AS yrs_left#627]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, pythonUDF0#634 AS yrs_left#627]\n",
      "+- ArrowEvalPython [remaining_yrs2(age#17)], [pythonUDF0#634], 200\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ratings#16, age#17, experience#18, family#19, mobile#20, pythonUDF0#634 AS yrs_left#627]\n",
      "+- ArrowEvalPython [remaining_yrs2(age#17)], [pythonUDF0#634], 200\n",
      "   +- FileScan csv [ratings#16,age#17,experience#18,family#19,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('yrs_left', remaining_yrs2(df['age'])).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, remaining_yrs3(age#17) AS yrs_left#799]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string, yrs_left: int\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, remaining_yrs3(age#17) AS yrs_left#799]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, pythonUDF0#806 AS yrs_left#799]\n",
      "+- ArrowEvalPython [remaining_yrs3(age#17)], [pythonUDF0#806], 200\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ratings#16, age#17, experience#18, family#19, mobile#20, pythonUDF0#806 AS yrs_left#799]\n",
      "+- ArrowEvalPython [remaining_yrs3(age#17)], [pythonUDF0#806], 200\n",
      "   +- FileScan csv [ratings#16,age#17,experience#18,family#19,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply pandas udf on dataframe\n",
    "df.withColumn('yrs_left', remaining_yrs3(df['age'])).explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, prod3(ratings#16, experience#18) AS product#808]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string, product: double\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, prod3(ratings#16, experience#18) AS product#808]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, pythonUDF0#815 AS product#808]\n",
      "+- ArrowEvalPython [prod3(ratings#16, experience#18)], [pythonUDF0#815], 200\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ratings#16, age#17, experience#18, family#19, mobile#20, pythonUDF0#815 AS product#808]\n",
      "+- ArrowEvalPython [prod3(ratings#16, experience#18)], [pythonUDF0#815], 200\n",
      "   +- FileScan csv [ratings#16,age#17,experience#18,family#19,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply pandas udf on multiple columns of dataframe\n",
    "df.withColumn(\"product\", prod3(df['ratings'],df['experience'])).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+-------+\n",
      "|ratings|age|experience|family| mobile|product|\n",
      "+-------+---+----------+------+-------+-------+\n",
      "|      3| 32|       9.0|     3|   Vivo|   27.0|\n",
      "|      3| 27|      13.0|     3|  Apple|   39.0|\n",
      "|      4| 22|       2.5|     0|Samsung|   10.0|\n",
      "|      4| 37|      16.5|     4|  Apple|   66.0|\n",
      "|      5| 27|       9.0|     1|     MI|   45.0|\n",
      "+-------+---+----------+------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql \n",
    "df.selectExpr('*', 'experience*ratings as product').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*, ('experience * 'ratings) AS product#816]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string, product: double\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, (experience#18 * cast(ratings#16 as double)) AS product#816]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ratings#16, age#17, experience#18, family#19, mobile#20, (experience#18 * cast(ratings#16 as double)) AS product#816]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ratings#16, age#17, experience#18, family#19, mobile#20, (experience#18 * cast(ratings#16 as double)) AS product#816]\n",
      "+- FileScan csv [ratings#16,age#17,experience#18,family#19,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql \n",
    "df.selectExpr('*', 'experience*ratings as product').explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the dataframe as single csv \n",
    "# mode='overwirte' 存在覆寫\n",
    "df.coalesce(1).write.csv('data/df_data.csv', header='True', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data into parquet format \n",
    "rtdf.write.parquet('data/retail_dataset_parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from parquet format \n",
    "rtdf2=spark.read.parquet('data/retail_dataset_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   549261|    21181|PLEASE ONE PERSON...|       2|4/7/2011 12:35|      2.1|     14903|United Kingdom|\n",
      "|   549261|    21903|  MAN FLU METAL SIGN|       2|4/7/2011 12:35|      2.1|     14903|United Kingdom|\n",
      "|   549261|    85150|LADIES & GENTLEME...|       4|4/7/2011 12:35|     2.55|     14903|United Kingdom|\n",
      "|   549261|    21908|CHOCOLATE THIS WA...|       2|4/7/2011 12:35|      2.1|     14903|United Kingdom|\n",
      "|   549261|    22670|FRENCH WC SIGN BL...|       3|4/7/2011 12:35|     1.25|     14903|United Kingdom|\n",
      "|   549261|    22197|SMALL POPCORN HOLDER|       7|4/7/2011 12:35|     0.85|     14903|United Kingdom|\n",
      "|   549261|    82581|   TOILET METAL SIGN|       4|4/7/2011 12:35|     0.55|     14903|United Kingdom|\n",
      "|   549261|    22674|FRENCH TOILET SIG...|       4|4/7/2011 12:35|     1.25|     14903|United Kingdom|\n",
      "|   549261|    22855|  FINE WICKER HEART |       6|4/7/2011 12:35|     1.25|     14903|United Kingdom|\n",
      "|   549261|    22729|ALARM CLOCK BAKEL...|       4|4/7/2011 12:35|     3.75|     14903|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rtdf2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlowerBound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mupperBound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnumPartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpredicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Construct a :class:`DataFrame` representing the database table named ``table``\n",
       "accessible via JDBC URL ``url`` and connection ``properties``.\n",
       "\n",
       "Partitions of the table will be retrieved in parallel if either ``column`` or\n",
       "``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
       "is needed when ``column`` is specified.\n",
       "\n",
       "If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
       "\n",
       ".. versionadded:: 1.4.0\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "url : str\n",
       "    a JDBC URL of the form ``jdbc:subprotocol:subname``\n",
       "table : str\n",
       "    the name of the table\n",
       "column : str, optional\n",
       "    the name of a column of numeric, date, or timestamp type\n",
       "    that will be used for partitioning;\n",
       "    if this parameter is specified, then ``numPartitions``, ``lowerBound``\n",
       "    (inclusive), and ``upperBound`` (exclusive) will form partition strides\n",
       "    for generated WHERE clause expressions used to split the column\n",
       "    ``column`` evenly\n",
       "lowerBound : str or int, optional\n",
       "    the minimum value of ``column`` used to decide partition stride\n",
       "upperBound : str or int, optional\n",
       "    the maximum value of ``column`` used to decide partition stride\n",
       "numPartitions : int, optional\n",
       "    the number of partitions\n",
       "predicates : list, optional\n",
       "    a list of expressions suitable for inclusion in WHERE clauses;\n",
       "    each one defines one partition of the :class:`DataFrame`\n",
       "properties : dict, optional\n",
       "    a dictionary of JDBC database connection arguments. Normally at\n",
       "    least properties \"user\" and \"password\" with their corresponding values.\n",
       "    For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Don't create too many partitions in parallel on a large cluster;\n",
       "otherwise Spark might crash your external database systems.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.jdbc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = spark.read.csv('data/winequality_white.csv',sep=';',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fixed_acidity',\n",
       " 'volatile_acidity',\n",
       " 'citric_acid',\n",
       " 'residual_sugar',\n",
       " 'chlorides',\n",
       " 'free_sulfur_ dioxide',\n",
       " 'total_sulfur_dioxide',\n",
       " 'density',\n",
       " 'pH',\n",
       " 'sulphates',\n",
       " 'alcohol',\n",
       " 'quality']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns of dataframe\n",
    "wdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 12)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataset\n",
    "wdf.count(),len(wdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed_acidity: double (nullable = true)\n",
      " |-- volatile_acidity: double (nullable = true)\n",
      " |-- citric_acid: double (nullable = true)\n",
      " |-- residual_sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free_sulfur_ dioxide: double (nullable = true)\n",
      " |-- total_sulfur_dioxide: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- quality: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print dataframe schema\n",
    "wdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+--------------------+--------------------+-------+----+---------+-------+-------+\n",
      "|fixed_acidity|volatile_acidity|citric_acid|residual_sugar|chlorides|free_sulfur_ dioxide|total_sulfur_dioxide|density|  pH|sulphates|alcohol|quality|\n",
      "+-------------+----------------+-----------+--------------+---------+--------------------+--------------------+-------+----+---------+-------+-------+\n",
      "|          7.0|            0.27|       0.36|          20.7|    0.045|                45.0|               170.0|  1.001| 3.0|     0.45|    8.8|      6|\n",
      "|          6.3|             0.3|       0.34|           1.6|    0.049|                14.0|               132.0|  0.994| 3.3|     0.49|    9.5|      6|\n",
      "|          8.1|            0.28|        0.4|           6.9|     0.05|                30.0|                97.0| 0.9951|3.26|     0.44|   10.1|      6|\n",
      "|          7.2|            0.23|       0.32|           8.5|    0.058|                47.0|               186.0| 0.9956|3.19|      0.4|    9.9|      6|\n",
      "|          7.2|            0.23|       0.32|           8.5|    0.058|                47.0|               186.0| 0.9956|3.19|      0.4|    9.9|      6|\n",
      "|          8.1|            0.28|        0.4|           6.9|     0.05|                30.0|                97.0| 0.9951|3.26|     0.44|   10.1|      6|\n",
      "|          6.2|            0.32|       0.16|           7.0|    0.045|                30.0|               136.0| 0.9949|3.18|     0.47|    9.6|      6|\n",
      "|          7.0|            0.27|       0.36|          20.7|    0.045|                45.0|               170.0|  1.001| 3.0|     0.45|    8.8|      6|\n",
      "|          6.3|             0.3|       0.34|           1.6|    0.049|                14.0|               132.0|  0.994| 3.3|     0.49|    9.5|      6|\n",
      "|          8.1|            0.22|       0.43|           1.5|    0.044|                28.0|               129.0| 0.9938|3.22|     0.45|   11.0|      6|\n",
      "+-------------+----------------+-----------+--------------+---------+--------------------+--------------------+-------+----+---------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display fisrt few rows of dataframe\n",
    "#wdf.show()\n",
    "wdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+--------------------+\n",
      "|summary|                 pH|          sulphates|           chlorides|\n",
      "+-------+-------------------+-------------------+--------------------+\n",
      "|  count|               4898|               4898|                4898|\n",
      "|   mean| 3.1882666394446693| 0.4898468762760325|  0.0457723560636995|\n",
      "| stddev|0.15100059961506673|0.11412583394883222|0.021847968093728805|\n",
      "|    min|               2.72|               0.22|               0.009|\n",
      "|    25%|               3.09|               0.41|               0.036|\n",
      "|    50%|               3.18|               0.47|               0.043|\n",
      "|    75%|               3.28|               0.55|                0.05|\n",
      "|    max|               3.82|               1.08|               0.346|\n",
      "+-------+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wdf.select('pH','sulphates','chlorides').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas udf function\n",
    "# to detect outlier\n",
    "@pandas_udf('int')\n",
    "def outliers_iqr(val: pd.Series) -> pd.Series:\n",
    "    quartile_1, quartile_3 = np.percentile(val, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    \n",
    "    return pd.Series(np.where((val > upper_bound) | (val < lower_bound),1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf2 = wdf.withColumn('pH_out', outliers_iqr(wdf['pH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache =>當資料顯示不出時用\n",
    "wdf2.cache()\n",
    "wdf2.createOrReplaceTempView(\"wdf2Table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mwdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
       "\n",
       ".. versionadded:: 1.3.0\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wdf2.cache?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed_acidity: double (nullable = true)\n",
      " |-- volatile_acidity: double (nullable = true)\n",
      " |-- citric_acid: double (nullable = true)\n",
      " |-- residual_sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free_sulfur_ dioxide: double (nullable = true)\n",
      " |-- total_sulfur_dioxide: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- quality: integer (nullable = true)\n",
      " |-- pH_out: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wdf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  pH|pH_out|\n",
      "+----+------+\n",
      "|3.69|     1|\n",
      "|3.63|     1|\n",
      "|3.72|     1|\n",
      "|3.61|     1|\n",
      "|3.64|     1|\n",
      "|3.64|     1|\n",
      "|3.72|     1|\n",
      "|3.72|     1|\n",
      "|3.58|     1|\n",
      "|3.58|     1|\n",
      "+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wdf2.select('pH','pH_out').filter('pH_out==1').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  pH|pH_out|\n",
      "+----+------+\n",
      "|3.69|     1|\n",
      "|3.63|     1|\n",
      "|3.72|     1|\n",
      "|3.61|     1|\n",
      "|3.64|     1|\n",
      "|3.64|     1|\n",
      "|3.72|     1|\n",
      "|3.72|     1|\n",
      "|3.58|     1|\n",
      "|3.58|     1|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select pH,pH_out from wdf2Table where pH_out=1 limit 10''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存檔前要用 cache => 不然會沒辦法存\n",
    "# save the data into parquet format \n",
    "wdf2.write.csv('data/wdf2', header='True', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv Dataset\n",
    "wdf3=spark.read.csv('data/wdf2',inferSchema=True,header=True)\n",
    "wdf3.createOrReplaceTempView(\"wdf4Table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+--------------------+--------------------+-------+----+---------+-------+-------+------+\n",
      "|fixed_acidity|volatile_acidity|citric_acid|residual_sugar|chlorides|free_sulfur_ dioxide|total_sulfur_dioxide|density|  pH|sulphates|alcohol|quality|pH_out|\n",
      "+-------------+----------------+-----------+--------------+---------+--------------------+--------------------+-------+----+---------+-------+-------+------+\n",
      "|          6.0|            0.27|       0.28|           4.8|    0.063|                31.0|               201.0| 0.9964|3.69|     0.71|   10.0|      5|     1|\n",
      "|          5.5|           0.485|        0.0|           1.5|    0.065|                 8.0|               103.0|  0.994|3.63|      0.4|    9.7|      4|     1|\n",
      "|          5.9|            0.21|       0.28|           4.6|    0.053|                40.0|               199.0| 0.9964|3.72|      0.7|   10.0|      4|     1|\n",
      "|          6.0|             0.1|       0.24|           1.1|    0.041|                15.0|                65.0| 0.9927|3.61|     0.61|   10.3|      7|     1|\n",
      "|          6.0|            0.24|       0.27|           1.9|    0.048|                40.0|               170.0| 0.9938|3.64|     0.54|   10.0|      7|     1|\n",
      "|          6.0|            0.24|       0.27|           1.9|    0.048|                40.0|               170.0| 0.9938|3.64|     0.54|   10.0|      7|     1|\n",
      "|          7.3|           0.205|       0.31|           1.7|     0.06|                34.0|               110.0| 0.9963|3.72|     0.69|   10.5|      6|     1|\n",
      "|          7.3|           0.205|       0.31|           1.7|     0.06|                34.0|               110.0| 0.9963|3.72|     0.69|   10.5|      6|     1|\n",
      "|          5.9|            0.26|        0.3|           1.0|    0.036|                38.0|               114.0| 0.9928|3.58|     0.48|    9.4|      5|     1|\n",
      "|          6.5|            0.25|       0.35|          12.0|    0.055|                47.0|               179.0|  0.998|3.58|     0.47|   10.0|      5|     1|\n",
      "|          6.1|            0.16|       0.27|          12.6|    0.064|                63.0|               162.0| 0.9994|3.66|     0.43|    8.9|      5|     1|\n",
      "|          5.3|            0.24|       0.33|           1.3|    0.033|                25.0|                97.0| 0.9906|3.59|     0.38|   11.0|      8|     1|\n",
      "|          9.7|            0.24|       0.45|           1.2|    0.033|                11.0|                59.0| 0.9926|2.74|     0.47|   10.8|      6|     1|\n",
      "|          5.3|            0.26|       0.23|          5.15|    0.034|                48.0|               160.0| 0.9952|3.82|     0.51|   10.5|      7|     1|\n",
      "|          6.4|            0.22|       0.34|           1.8|    0.057|                29.0|               104.0| 0.9959|3.81|     0.57|   10.3|      6|     1|\n",
      "+-------------+----------------+-----------+--------------+---------+--------------------+--------------------+-------+----+---------+-------+-------+------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wdf3.filter('pH_out==1').show(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
