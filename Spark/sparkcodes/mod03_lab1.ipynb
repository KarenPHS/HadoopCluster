{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 執行 row 2、4即可以跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkSession => native 不需要，執行 row 2、4即可以跑\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn  \n",
    "from pyspark.sql.types import StringType,DoubleType,IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for standalone(lightweight) pyspark\n",
    "# create spar session object\n",
    "spark=SparkSession.builder.appName('data_processing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySparkShell'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.appName # 代表正在用互動式的spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv Dataset \n",
    "df=spark.read.csv('data/sample_data.csv',inferSchema=True,header=True) \n",
    "# inferSchema,header預設為 False，inferSchema=True 才會檢查欄位資料型態，不檢查會把數字當文字\n",
    "# header=True 才會判斷第一列是否為標題\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string\n",
      "Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [ratings#16,age#17,experience#18,family#19,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.explain() \n",
    "df.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ratings', 'age', 'experience', 'family', 'mobile']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns of dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of columns\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of records in dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataset\n",
    "df.count(),len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ratings: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: double (nullable = true)\n",
      " |-- family: integer (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print dataframe schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+\n",
      "|ratings|age|experience|family| mobile|\n",
      "+-------+---+----------+------+-------+\n",
      "|      3| 32|       9.0|     3|   Vivo|\n",
      "|      3| 27|      13.0|     3|  Apple|\n",
      "|      4| 22|       2.5|     0|Samsung|\n",
      "|      4| 37|      16.5|     4|  Apple|\n",
      "|      5| 27|       9.0|     1|     MI|\n",
      "|      4| 27|       9.0|     0|   Oppo|\n",
      "|      5| 37|      23.0|     5|   Vivo|\n",
      "|      5| 37|      23.0|     5|Samsung|\n",
      "|      3| 22|       2.5|     0|  Apple|\n",
      "|      3| 27|       6.0|     0|     MI|\n",
      "+-------+---+----------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display fisrt few rows of dataframe => 預設20筆\n",
    "# df.show()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+\n",
      "|ratings|age|experience|family| mobile|\n",
      "+-------+---+----------+------+-------+\n",
      "|      3| 32|       9.0|     3|   Vivo|\n",
      "|      3| 27|      13.0|     3|  Apple|\n",
      "|      4| 22|       2.5|     0|Samsung|\n",
      "|      4| 37|      16.5|     4|  Apple|\n",
      "|      5| 27|       9.0|     1|     MI|\n",
      "|      4| 27|       9.0|     0|   Oppo|\n",
      "|      5| 37|      23.0|     5|   Vivo|\n",
      "|      5| 37|      23.0|     5|Samsung|\n",
      "|      3| 22|       2.5|     0|  Apple|\n",
      "|      3| 27|       6.0|     0|     MI|\n",
      "+-------+---+----------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# truncate=True => If set to True, truncate strings longer than 20 chars by default，truncate=3 :=> 每欄 3個字母\n",
    "df.show(10, truncate=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ratings=3, age=32, experience=9.0, family=3, mobile='Vivo'),\n",
       " Row(ratings=3, age=27, experience=13.0, family=3, mobile='Apple'),\n",
       " Row(ratings=4, age=22, experience=2.5, family=0, mobile='Samsung'),\n",
       " Row(ratings=4, age=37, experience=16.5, family=4, mobile='Apple'),\n",
       " Row(ratings=5, age=27, experience=9.0, family=1, mobile='MI')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display fisrt 5 rows of dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ratings=2, age=32, experience=16.5, family=2, mobile='Oppo'),\n",
       " Row(ratings=3, age=27, experience=6.0, family=0, mobile='MI'),\n",
       " Row(ratings=3, age=27, experience=6.0, family=0, mobile='MI'),\n",
       " Row(ratings=4, age=22, experience=6.0, family=1, mobile='Oppo'),\n",
       " Row(ratings=4, age=37, experience=6.0, family=0, mobile='Vivo')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display last 5 rows of dataframe\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ratings=3, age=32, experience=9.0, family=3, mobile='Vivo')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first row of dataframe\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+\n",
      "|ratings|age|experience|family| mobile|\n",
      "+-------+---+----------+------+-------+\n",
      "|      3| 32|       9.0|     3|   Vivo|\n",
      "|      3| 27|      13.0|     3|  Apple|\n",
      "|      4| 22|       2.5|     0|Samsung|\n",
      "+-------+---+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 因為有 df.createOrReplaceTempView(\"dfTable\") 才可以跑\n",
    "spark.sql('''select * from dfTable limit 3''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------+\n",
      "|summary|           ratings|               age|        experience|            family|mobile|\n",
      "+-------+------------------+------------------+------------------+------------------+------+\n",
      "|  count|                33|                33|                33|                33|    33|\n",
      "|   mean|3.5757575757575757|30.484848484848484|10.303030303030303|1.8181818181818181|  null|\n",
      "| stddev|1.1188806636071336|  6.18527087180309| 6.770731351213326|1.8448330794164254|  null|\n",
      "|    min|                 1|                22|               2.5|                 0| Apple|\n",
      "|    max|                 5|                42|              23.0|                 5|  Vivo|\n",
      "+-------+------------------+------------------+------------------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# info about dataframe\n",
    "# df.describe() => 只執行 plan\n",
    "df.describe().show() # show() => 才會 actoin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------+\n",
      "|summary|           ratings|               age|        experience|            family|mobile|\n",
      "+-------+------------------+------------------+------------------+------------------+------+\n",
      "|  count|                33|                33|                33|                33|    33|\n",
      "|   mean|3.5757575757575757|30.484848484848484|10.303030303030303|1.8181818181818181|  null|\n",
      "| stddev|1.1188806636071336|  6.18527087180309| 6.770731351213326|1.8448330794164254|  null|\n",
      "|    min|                 1|                22|               2.5|                 0| Apple|\n",
      "|    25%|                 3|                27|               6.0|                 0|  null|\n",
      "|    50%|                 4|                27|               6.0|                 1|  null|\n",
      "|    75%|                 4|                37|              16.5|                 3|  null|\n",
      "|    max|                 5|                42|              23.0|                 5|  Vivo|\n",
      "+-------+------------------+------------------+------------------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# info about dataframe\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age| mobile|\n",
      "+---+-------+\n",
      "| 32|   Vivo|\n",
      "| 27|  Apple|\n",
      "| 22|Samsung|\n",
      "| 37|  Apple|\n",
      "| 27|     MI|\n",
      "+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select only 2 columns\n",
    "df.select('age','mobile').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age| mobile|\n",
      "+---+-------+\n",
      "| 32|   Vivo|\n",
      "| 27|  Apple|\n",
      "| 22|Samsung|\n",
      "| 37|  Apple|\n",
      "| 27|     MI|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql\n",
    "spark.sql('select age, mobile from dfTable limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('age, None), unresolvedalias('mobile, None)]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "age: int, mobile: string\n",
      "Project [age#17, mobile#20]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [age#17, mobile#20]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [age#17,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age','mobile').explain(extended=True) # 最佳化引擎，過程不一樣 Physical Plan 相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['age, 'mobile]\n",
      "+- 'UnresolvedRelation [dfTable], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "age: int, mobile: string\n",
      "Project [age#17, mobile#20]\n",
      "+- SubqueryAlias dftable\n",
      "   +- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [age#17, mobile#20]\n",
      "+- Relation[ratings#16,age#17,experience#18,family#19,mobile#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [age#17,mobile#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select age, mobile from dfTable').explain(extended=True) # 最佳化引擎，過程不一樣 Physical Plan 相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2b9ade4195ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ratings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'family'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     fn.column('mobile'))\\\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mcolumn\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# use different pyspark sql functions\n",
    "df.select(\n",
    "    fn.expr('ratings'), \n",
    "    fn.col('family'), \n",
    "    fn.column('mobile'))\\\n",
    ".show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns a :class:`~pyspark.sql.Column` based on the given column name.'\n",
       "\n",
       ".. versionadded:: 1.3\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/sql/functions.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn.column? # => 無法傳字串資料型態(目前參數打錯，等版更) return col(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|ratings|family| mobile|\n",
      "+-------+------+-------+\n",
      "|      3|     3|   Vivo|\n",
      "|      3|     3|  Apple|\n",
      "|      4|     0|Samsung|\n",
      "+-------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use different pyspark sql functions\n",
    "df.select(\n",
    "    fn.expr('ratings'), \n",
    "    fn.col('family'), \n",
    "    fn.col('mobile'))\\\n",
    ".show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|age1|\n",
      "+----+\n",
      "|  33|\n",
      "|  28|\n",
      "|  23|\n",
      "|  38|\n",
      "|  28|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(fn.expr('age+1 AS age1')).show(5) # fn.expr 效能最好，所以實作上可取代 fn.col、fn.column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|age1|\n",
      "+----+\n",
      "|  33|\n",
      "|  28|\n",
      "|  23|\n",
      "|  38|\n",
      "|  28|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('age+1 AS age1').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+------+\n",
      "|ratings|age|experience|family| mobile|over30|\n",
      "+-------+---+----------+------+-------+------+\n",
      "|      3| 32|       9.0|     3|   Vivo|  true|\n",
      "|      3| 27|      13.0|     3|  Apple| false|\n",
      "|      4| 22|       2.5|     0|Samsung| false|\n",
      "|      4| 37|      16.5|     4|  Apple|  true|\n",
      "|      5| 27|       9.0|     1|     MI| false|\n",
      "+-------+---+----------+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.selectExpr() 等同於 df.select(fn.expr())\n",
    "df.selectExpr(\n",
    "'*',  # all original columns\n",
    "'(age>=30) as over30')\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+------+\n",
      "|ratings|age|experience|family| mobile|over30|\n",
      "+-------+---+----------+------+-------+------+\n",
      "|      3| 32|       9.0|     3|   Vivo|  true|\n",
      "|      3| 27|      13.0|     3|  Apple| false|\n",
      "|      4| 22|       2.5|     0|Samsung| false|\n",
      "|      4| 37|      16.5|     4|  Apple|  true|\n",
      "|      5| 27|       9.0|     1|     MI| false|\n",
      "+-------+---+----------+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT *, (age>=30) as over30 FROM dfTable LIMIT 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*, ('age >= 30) AS over30#446]\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string, over30: boolean\n",
      "Project [ratings#409, age#410, experience#411, family#412, mobile#413, (age#410 >= 30) AS over30#446]\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ratings#409, age#410, experience#411, family#412, mobile#413, (age#410 >= 30) AS over30#446]\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ratings#409, age#410, experience#411, family#412, mobile#413, (age#410 >= 30) AS over30#446]\n",
      "+- FileScan csv [ratings#409,age#410,experience#411,family#412,mobile#413] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "'*',  # all original columns\n",
    "'(age>=30) as over30')\\\n",
    ".explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*, ('age >= 30) AS over30#453]\n",
      "+- 'UnresolvedRelation [dfTable], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string, over30: boolean\n",
      "Project [ratings#409, age#410, experience#411, family#412, mobile#413, (age#410 >= 30) AS over30#453]\n",
      "+- SubqueryAlias dftable\n",
      "   +- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ratings#409, age#410, experience#411, family#412, mobile#413, (age#410 >= 30) AS over30#453]\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ratings#409, age#410, experience#411, family#412, mobile#413, (age#410 >= 30) AS over30#453]\n",
      "+- FileScan csv [ratings#409,age#410,experience#411,family#412,mobile#413] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT *, (age>=30) as over30 FROM dfTable').explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+----------------+\n",
      "|ratings|age|experience|family|mobile |age_after_10_yrs|\n",
      "+-------+---+----------+------+-------+----------------+\n",
      "|3      |32 |9.0       |3     |Vivo   |42              |\n",
      "|3      |27 |13.0      |3     |Apple  |37              |\n",
      "|4      |22 |2.5       |0     |Samsung|32              |\n",
      "|4      |37 |16.5      |4     |Apple  |47              |\n",
      "|5      |27 |9.0       |1     |MI     |37              |\n",
      "|4      |27 |9.0       |0     |Oppo   |37              |\n",
      "|5      |37 |23.0      |5     |Vivo   |47              |\n",
      "|5      |37 |23.0      |5     |Samsung|47              |\n",
      "|3      |22 |2.5       |0     |Apple  |32              |\n",
      "|3      |27 |6.0       |0     |MI     |37              |\n",
      "+-------+---+----------+------+-------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with column\n",
    "df.withColumn('age_after_10_yrs',(df['age']+10)).show(10,False) # show( ,False) => 目前差在欄位靠左靠右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+----------+\n",
      "|ratings|age|experience|family|mobile |age_double|\n",
      "+-------+---+----------+------+-------+----------+\n",
      "|3      |32 |9.0       |3     |Vivo   |32.0      |\n",
      "|3      |27 |13.0      |3     |Apple  |27.0      |\n",
      "|4      |22 |2.5       |0     |Samsung|22.0      |\n",
      "|4      |37 |16.5      |4     |Apple  |37.0      |\n",
      "|5      |27 |9.0       |1     |MI     |27.0      |\n",
      "|4      |27 |9.0       |0     |Oppo   |27.0      |\n",
      "|5      |37 |23.0      |5     |Vivo   |37.0      |\n",
      "|5      |37 |23.0      |5     |Samsung|37.0      |\n",
      "|3      |22 |2.5       |0     |Apple  |22.0      |\n",
      "|3      |27 |6.0       |0     |MI     |27.0      |\n",
      "+-------+---+----------+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert data type\n",
    "df.withColumn('age_double',df['age'].cast(DoubleType())).show(10,False) # show( ,False) => 目前差在欄位靠左靠右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+----------+\n",
      "|ratings|age|experience|family| mobile|age_double|\n",
      "+-------+---+----------+------+-------+----------+\n",
      "|      3| 32|       9.0|     3|   Vivo|      32.0|\n",
      "|      3| 27|      13.0|     3|  Apple|      27.0|\n",
      "|      4| 22|       2.5|     0|Samsung|      22.0|\n",
      "|      4| 37|      16.5|     4|  Apple|      37.0|\n",
      "|      5| 27|       9.0|     1|     MI|      27.0|\n",
      "+-------+---+----------+------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use selectExpr method\n",
    "df.selectExpr(\n",
    "'*',  # all original columns\n",
    "'cast(age as double) as age_double')\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [ratings#409, age#410, experience#411, family#412, mobile#413, cast(age#410 as double) AS age_double#652]\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string, age_double: double\n",
      "Project [ratings#409, age#410, experience#411, family#412, mobile#413, cast(age#410 as double) AS age_double#652]\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ratings#409, age#410, experience#411, family#412, mobile#413, cast(age#410 as double) AS age_double#652]\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ratings#409, age#410, experience#411, family#412, mobile#413, cast(age#410 as double) AS age_double#652]\n",
      "+- FileScan csv [ratings#409,age#410,experience#411,family#412,mobile#413] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert data type\n",
    "df.withColumn('age_double',df['age'].cast(DoubleType())).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*, cast('age as double) AS age_double#645]\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string, age_double: double\n",
      "Project [ratings#409, age#410, experience#411, family#412, mobile#413, cast(age#410 as double) AS age_double#645]\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ratings#409, age#410, experience#411, family#412, mobile#413, cast(age#410 as double) AS age_double#645]\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ratings#409, age#410, experience#411, family#412, mobile#413, cast(age#410 as double) AS age_double#645]\n",
      "+- FileScan csv [ratings#409,age#410,experience#411,family#412,mobile#413] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use selectExpr method\n",
    "df.selectExpr(\n",
    "'*',  # all original columns\n",
    "'cast(age as double) as age_double')\\\n",
    ".explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "https://spark.apache.org/docs/latest/api/sql/\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|ratings|age|experience|family|\n",
      "+-------+---+----------+------+\n",
      "|      3| 32|       9.0|     3|\n",
      "|      3| 27|      13.0|     3|\n",
      "|      4| 22|       2.5|     0|\n",
      "|      4| 37|      16.5|     4|\n",
      "|      5| 27|       9.0|     1|\n",
      "+-------+---+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ** df_new 重複使用 ** => 控制記憶體\n",
    "# delete a column \n",
    "df_new=df.drop('mobile')\n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|ratings|experience|family|\n",
      "+-------+----------+------+\n",
      "|      3|       9.0|     3|\n",
      "|      3|      13.0|     3|\n",
      "|      4|       2.5|     0|\n",
      "|      4|      16.5|     4|\n",
      "|      5|       9.0|     1|\n",
      "+-------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ** df_new 重複使用 ** => 控制記憶體\n",
    "df_new=df.drop('age', 'mobile')\n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+------+\n",
      "|ratings|age|experience|family|mobile|\n",
      "+-------+---+----------+------+------+\n",
      "|      3| 32|       9.0|     3|  Vivo|\n",
      "|      5| 37|      23.0|     5|  Vivo|\n",
      "|      4| 37|       6.0|     0|  Vivo|\n",
      "|      5| 37|      13.0|     1|  Vivo|\n",
      "|      4| 37|       6.0|     0|  Vivo|\n",
      "+-------+---+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the records \n",
    "df.filter(df['mobile']=='Vivo').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Filter (mobile#413 = Vivo)\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string\n",
      "Filter (mobile#413 = Vivo)\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(mobile#413) AND (mobile#413 = Vivo))\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(mobile#413) AND (mobile#413 = Vivo))\n",
      "+- FileScan csv [ratings#409,age#410,experience#411,family#412,mobile#413] Batched: false, DataFilters: [isnotnull(mobile#413), (mobile#413 = Vivo)], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(mobile), EqualTo(mobile,Vivo)], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the records \n",
    "df.filter(df['mobile']=='Vivo').explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "|age|ratings|mobile|\n",
      "+---+-------+------+\n",
      "| 32|      3|  Vivo|\n",
      "| 37|      5|  Vivo|\n",
      "| 37|      4|  Vivo|\n",
      "| 37|      5|  Vivo|\n",
      "| 37|      4|  Vivo|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the records \n",
    "df.filter(df['mobile']=='Vivo').select('age','ratings','mobile').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "|age|ratings|mobile|\n",
      "+---+-------+------+\n",
      "| 32|      3|  Vivo|\n",
      "| 37|      5|  Vivo|\n",
      "| 37|      4|  Vivo|\n",
      "| 37|      5|  Vivo|\n",
      "| 37|      4|  Vivo|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the records \n",
    "df.filter(\"mobile=='Vivo'\").select('age','ratings','mobile').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "|age|ratings|mobile|\n",
      "+---+-------+------+\n",
      "| 32|      3|  Vivo|\n",
      "| 37|      5|  Vivo|\n",
      "| 37|      4|  Vivo|\n",
      "| 37|      5|  Vivo|\n",
      "| 37|      4|  Vivo|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the records with spark sql \n",
    "spark.sql(\"\"\"select age, ratings, mobile from dfTable where mobile=='Vivo'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+------+\n",
      "|ratings|age|experience|family|mobile|\n",
      "+-------+---+----------+------+------+\n",
      "|      5| 37|      23.0|     5|  Vivo|\n",
      "|      5| 37|      13.0|     1|  Vivo|\n",
      "+-------+---+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the multiple conditions\n",
    "df.filter(df['mobile']=='Vivo').filter(df['experience'] >10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+------+\n",
      "|ratings|age|experience|family|mobile|\n",
      "+-------+---+----------+------+------+\n",
      "|      5| 37|      23.0|     5|  Vivo|\n",
      "|      5| 37|      13.0|     1|  Vivo|\n",
      "+-------+---+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the multiple conditions\n",
    "df.filter((df['mobile']=='Vivo')&(df['experience'] >10)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+------+\n",
      "|ratings|age|experience|family|mobile|\n",
      "+-------+---+----------+------+------+\n",
      "|      5| 37|      23.0|     5|  Vivo|\n",
      "|      5| 37|      13.0|     1|  Vivo|\n",
      "+-------+---+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the multiple conditions\n",
    "# SQL Styles\n",
    "df.filter(\"mobile=='Vivo' and experience>10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+------+\n",
      "|ratings|age|experience|family|mobile|\n",
      "+-------+---+----------+------+------+\n",
      "|      5| 37|      23.0|     5|  Vivo|\n",
      "|      5| 37|      13.0|     1|  Vivo|\n",
      "+-------+---+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the records with spark sql \n",
    "spark.sql('''select * from dfTable where mobile=='Vivo' and experience>10''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Plan Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter (experience#411 > 10)\n",
      "+- Filter (mobile#413 = Vivo)\n",
      "   +- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string\n",
      "Filter (experience#411 > cast(10 as double))\n",
      "+- Filter (mobile#413 = Vivo)\n",
      "   +- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (((isnotnull(mobile#413) AND isnotnull(experience#411)) AND (mobile#413 = Vivo)) AND (experience#411 > 10.0))\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (((isnotnull(mobile#413) AND isnotnull(experience#411)) AND (mobile#413 = Vivo)) AND (experience#411 > 10.0))\n",
      "+- FileScan csv [ratings#409,age#410,experience#411,family#412,mobile#413] Batched: false, DataFilters: [isnotnull(mobile#413), isnotnull(experience#411), (mobile#413 = Vivo), (experience#411 > 10.0)], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(mobile), IsNotNull(experience), EqualTo(mobile,Vivo), GreaterThan(experience,10.0)], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the multiple conditions\n",
    "df.filter(df['mobile']=='Vivo').filter(df['experience'] >10).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ((mobile#413 = Vivo) AND (experience#411 > 10))\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string\n",
      "Filter ((mobile#413 = Vivo) AND (experience#411 > cast(10 as double)))\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (((isnotnull(mobile#413) AND isnotnull(experience#411)) AND (mobile#413 = Vivo)) AND (experience#411 > 10.0))\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (((isnotnull(mobile#413) AND isnotnull(experience#411)) AND (mobile#413 = Vivo)) AND (experience#411 > 10.0))\n",
      "+- FileScan csv [ratings#409,age#410,experience#411,family#412,mobile#413] Batched: false, DataFilters: [isnotnull(mobile#413), isnotnull(experience#411), (mobile#413 = Vivo), (experience#411 > 10.0)], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(mobile), IsNotNull(experience), EqualTo(mobile,Vivo), GreaterThan(experience,10.0)], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the multiple conditions\n",
    "df.filter((df['mobile']=='Vivo')&(df['experience'] >10)).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter (('mobile = Vivo) AND ('experience > 10))\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string\n",
      "Filter ((mobile#413 = Vivo) AND (experience#411 > cast(10 as double)))\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (((isnotnull(mobile#413) AND isnotnull(experience#411)) AND (mobile#413 = Vivo)) AND (experience#411 > 10.0))\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (((isnotnull(mobile#413) AND isnotnull(experience#411)) AND (mobile#413 = Vivo)) AND (experience#411 > 10.0))\n",
      "+- FileScan csv [ratings#409,age#410,experience#411,family#412,mobile#413] Batched: false, DataFilters: [isnotnull(mobile#413), isnotnull(experience#411), (mobile#413 = Vivo), (experience#411 > 10.0)], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(mobile), IsNotNull(experience), EqualTo(mobile,Vivo), GreaterThan(experience,10.0)], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the multiple conditions\n",
    "df.filter(\"mobile=='Vivo' and experience>10\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter (('mobile = Vivo) AND ('experience > 10))\n",
      "   +- 'UnresolvedRelation [dfTable], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string\n",
      "Project [ratings#409, age#410, experience#411, family#412, mobile#413]\n",
      "+- Filter ((mobile#413 = Vivo) AND (experience#411 > cast(10 as double)))\n",
      "   +- SubqueryAlias dftable\n",
      "      +- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (((isnotnull(mobile#413) AND isnotnull(experience#411)) AND (mobile#413 = Vivo)) AND (experience#411 > 10.0))\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (((isnotnull(mobile#413) AND isnotnull(experience#411)) AND (mobile#413 = Vivo)) AND (experience#411 > 10.0))\n",
      "+- FileScan csv [ratings#409,age#410,experience#411,family#412,mobile#413] Batched: false, DataFilters: [isnotnull(mobile#413), isnotnull(experience#411), (mobile#413 = Vivo), (experience#411 > 10.0)], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(mobile), IsNotNull(experience), EqualTo(mobile,Vivo), GreaterThan(experience,10.0)], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the records with spark sql \n",
    "spark.sql('''select * from dfTable where mobile=='Vivo' and experience>10''').explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinct Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| mobile|\n",
      "+-------+\n",
      "|     MI|\n",
      "|   Oppo|\n",
      "|Samsung|\n",
      "|   Vivo|\n",
      "|  Apple|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distinct Values in a columnc\n",
    "df.select('mobile').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Deduplicate [mobile#413]\n",
      "+- Project [mobile#413]\n",
      "   +- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mobile: string\n",
      "Deduplicate [mobile#413]\n",
      "+- Project [mobile#413]\n",
      "   +- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [mobile#413], [mobile#413]\n",
      "+- Project [mobile#413]\n",
      "   +- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[mobile#413], functions=[], output=[mobile#413])\n",
      "+- Exchange hashpartitioning(mobile#413, 200), ENSURE_REQUIREMENTS, [id=#275]\n",
      "   +- *(1) HashAggregate(keys=[mobile#413], functions=[], output=[mobile#413])\n",
      "      +- FileScan csv [mobile#413] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 開始 HashAggregate、hashpartitioning\n",
    "df.select('mobile').distinct().explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|ratings| mobile|\n",
      "+-------+-------+\n",
      "|      5|Samsung|\n",
      "|      5|     MI|\n",
      "|      4|  Apple|\n",
      "|      3|  Apple|\n",
      "|      2|   Oppo|\n",
      "|      4|   Oppo|\n",
      "|      1|     MI|\n",
      "|      3|     MI|\n",
      "|      4|Samsung|\n",
      "|      5|   Vivo|\n",
      "|      4|   Vivo|\n",
      "|      3|   Vivo|\n",
      "|      2|Samsung|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distinct Values in columns\n",
    "df.select('ratings', 'mobile').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct value count !!=> get a value \n",
    "df.select('mobile').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| mobile|\n",
      "+-------+\n",
      "|     MI|\n",
      "|   Oppo|\n",
      "|Samsung|\n",
      "|   Vivo|\n",
      "|  Apple|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql !!=> get a table\n",
    "spark.sql('''select distinct(mobile) from dfTable''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|counts|\n",
      "+------+\n",
      "|     5|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql\n",
    "spark.sql('''select count(distinct(mobile)) as counts from dfTable''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+\n",
      "|ratings|age|experience|family| mobile|\n",
      "+-------+---+----------+------+-------+\n",
      "|      4| 22|       6.0|     1|   Oppo|\n",
      "|      4| 22|       2.5|     0|Samsung|\n",
      "|      5| 22|       2.5|     0|Samsung|\n",
      "|      3| 22|       2.5|     0|  Apple|\n",
      "|      4| 22|       6.0|     1|   Oppo|\n",
      "|      5| 27|       6.0|     0|     MI|\n",
      "|      5| 27|       6.0|     2|Samsung|\n",
      "|      5| 27|       9.0|     1|     MI|\n",
      "|      4| 27|       6.0|     1|  Apple|\n",
      "|      3| 27|      13.0|     3|  Apple|\n",
      "+-------+---+----------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort age\n",
    "df.sort('age').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+\n",
      "|ratings|age|experience|family| mobile|\n",
      "+-------+---+----------+------+-------+\n",
      "|      1| 37|      23.0|     5|     MI|\n",
      "|      2| 27|       6.0|     2|   Oppo|\n",
      "|      2| 32|      16.5|     2|   Oppo|\n",
      "|      2| 42|      23.0|     2|   Oppo|\n",
      "|      2| 27|       9.0|     2|Samsung|\n",
      "|      2| 27|       6.0|     2|   Oppo|\n",
      "|      3| 22|       2.5|     0|  Apple|\n",
      "|      3| 27|       6.0|     0|     MI|\n",
      "|      3| 27|       6.0|     0|     MI|\n",
      "|      3| 27|       6.0|     0|     MI|\n",
      "+-------+---+----------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# order by ratings and family \n",
    "df.orderBy('ratings', 'family').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['ratings ASC NULLS FIRST, 'family ASC NULLS FIRST], true\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ratings: int, age: int, experience: double, family: int, mobile: string\n",
      "Sort [ratings#409 ASC NULLS FIRST, family#412 ASC NULLS FIRST], true\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [ratings#409 ASC NULLS FIRST, family#412 ASC NULLS FIRST], true\n",
      "+- Relation[ratings#409,age#410,experience#411,family#412,mobile#413] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Sort [ratings#409 ASC NULLS FIRST, family#412 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(ratings#409 ASC NULLS FIRST, family#412 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#286]\n",
      "   +- FileScan csv [ratings#409,age#410,experience#411,family#412,mobile#413] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/hadoop/sparkcodes/data/sample_data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ratings:int,age:int,experience:double,family:int,mobile:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# order by ratings and family => Exchange rangepartitioning\n",
    "df.orderBy('ratings', 'family').explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------+\n",
      "|ratings|age|experience|family| mobile|\n",
      "+-------+---+----------+------+-------+\n",
      "|      5| 37|      23.0|     5|   Vivo|\n",
      "|      5| 37|      23.0|     5|Samsung|\n",
      "|      1| 37|      23.0|     5|     MI|\n",
      "|      3| 42|      23.0|     5|     MI|\n",
      "|      2| 42|      23.0|     2|   Oppo|\n",
      "|      3| 37|      16.5|     5|  Apple|\n",
      "|      2| 32|      16.5|     2|   Oppo|\n",
      "|      4| 37|      16.5|     4|  Apple|\n",
      "|      3| 37|      16.5|     5|  Apple|\n",
      "|      3| 27|      13.0|     3|  Apple|\n",
      "+-------+---+----------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# descending order\n",
    "df.orderBy('experience', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+\n",
      "|ratings|age| mobile|\n",
      "+-------+---+-------+\n",
      "|      2| 42|   Oppo|\n",
      "|      3| 42|     MI|\n",
      "|      4| 37|  Apple|\n",
      "|      5| 37|   Vivo|\n",
      "|      3| 37|  Apple|\n",
      "|      1| 37|     MI|\n",
      "|      5| 37|Samsung|\n",
      "|      4| 37|Samsung|\n",
      "|      4| 37|   Vivo|\n",
      "|      3| 37|  Apple|\n",
      "+-------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql\n",
    "spark.sql(\"\"\"select ratings, age, mobile from dfTable \n",
    "        order by age desc\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "| name| age|weight|\n",
      "+-----+----+------+\n",
      "|Alice|  22|    52|\n",
      "| John|null|    68|\n",
      "| Mary|  24|    55|\n",
      "| Alan|null|  null|\n",
      "| Jane|  32|    48|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "data = [('Alice', 22, 52),\n",
    "        ('John', None , 68),\n",
    "        ('Mary', 24, 55),\n",
    "        ('Alan', None, None),\n",
    "        ('Jane', 32, 48)]\n",
    "\n",
    "emp=spark.createDataFrame(data, ['name', 'age', 'weight'])\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "| name|age|weight|\n",
      "+-----+---+------+\n",
      "|Alice| 22|    52|\n",
      "| John|  1|    68|\n",
      "| Mary| 24|    55|\n",
      "| Alan|  1|     1|\n",
      "| Jane| 32|    48|\n",
      "+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill null value\n",
    "emp.fillna(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "| name|age|weight|\n",
      "+-----+---+------+\n",
      "|Alice| 22|    52|\n",
      "| John| 30|    68|\n",
      "| Mary| 24|    55|\n",
      "| Alan| 30|    50|\n",
      "| Jane| 32|    48|\n",
      "+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill null value\n",
    "val={'age': 30, 'weight': 50}\n",
    "emp.fillna(val).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "| name|age|weight|\n",
      "+-----+---+------+\n",
      "|Alice| 22|    52|\n",
      "| Mary| 24|    55|\n",
      "| Jane| 32|    48|\n",
      "+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop null data\n",
    "emp.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "| name| age|weight|\n",
      "+-----+----+------+\n",
      "|Alice|  22|    52|\n",
      "| John|null|    68|\n",
      "| Mary|  24|    55|\n",
      "| Jane|  32|    48|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop null data， Alan 被丟掉\n",
    "emp.dropna(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0memp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'any'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns a new :class:`DataFrame` omitting rows with null values.\n",
       ":func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
       "\n",
       ".. versionadded:: 1.3.1\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "how : str, optional\n",
       "    'any' or 'all'.\n",
       "    If 'any', drop a row if it contains any nulls.\n",
       "    If 'all', drop a row only if all its values are null.\n",
       "thresh: int, optional\n",
       "    default None\n",
       "    If specified, drop rows that have less than `thresh` non-null values.\n",
       "    This overwrites the `how` parameter.\n",
       "subset : str, tuple or list, optional\n",
       "    optional list of column names to consider.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df4.na.drop().show()\n",
       "+---+------+-----+\n",
       "|age|height| name|\n",
       "+---+------+-----+\n",
       "| 10|    80|Alice|\n",
       "+---+------+-----+\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp.dropna?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n",
      "| name|age|country|\n",
      "+-----+---+-------+\n",
      "|Alice| 22|    USA|\n",
      "| John| 18|  Japan|\n",
      "| Mary| 24|Germany|\n",
      "|Alice| 22|    USA|\n",
      "| Jane| 24|Germany|\n",
      "+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "data = [('Alice', 22, 'USA'),\n",
    "        ('John', 18 , 'Japan'),\n",
    "        ('Mary', 24, 'Germany'),\n",
    "        ('Alice', 22, 'USA'),\n",
    "        ('Jane', 24, 'Germany')]\n",
    "\n",
    "cust=spark.createDataFrame(data, ['name', 'age', 'country'])\n",
    "cust.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n",
      "| name|age|country|\n",
      "+-----+---+-------+\n",
      "|Alice| 22|    USA|\n",
      "| Jane| 24|Germany|\n",
      "| Mary| 24|Germany|\n",
      "| John| 18|  Japan|\n",
      "+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop duplicate rows\n",
    "cust.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mcust\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return a new :class:`DataFrame` with duplicate rows removed,\n",
       "optionally only considering certain columns.\n",
       "\n",
       "For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
       ":class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
       "duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
       "be and system will accordingly limit the state. In addition, too late data older than\n",
       "watermark will be dropped to avoid any possibility of duplicates.\n",
       "\n",
       ":func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
       "\n",
       ".. versionadded:: 1.4.0\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from pyspark.sql import Row\n",
       ">>> df = sc.parallelize([ \\\n",
       "...     Row(name='Alice', age=5, height=80), \\\n",
       "...     Row(name='Alice', age=5, height=80), \\\n",
       "...     Row(name='Alice', age=10, height=80)]).toDF()\n",
       ">>> df.dropDuplicates().show()\n",
       "+-----+---+------+\n",
       "| name|age|height|\n",
       "+-----+---+------+\n",
       "|Alice|  5|    80|\n",
       "|Alice| 10|    80|\n",
       "+-----+---+------+\n",
       "\n",
       ">>> df.dropDuplicates(['name', 'height']).show()\n",
       "+-----+---+------+\n",
       "| name|age|height|\n",
       "+-----+---+------+\n",
       "|Alice|  5|    80|\n",
       "+-----+---+------+\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/spark/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cust.dropDuplicates? # 沒有 keep 可以設"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n",
      "| name|age|country|\n",
      "+-----+---+-------+\n",
      "|Alice| 22|    USA|\n",
      "| John| 18|  Japan|\n",
      "| Mary| 24|Germany|\n",
      "+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop duplicate rows base on some columns\n",
    "cust.dropDuplicates(subset=['age', 'country']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n",
      "| name|age|country|\n",
      "+-----+---+-------+\n",
      "|Alice| 22|    USA|\n",
      "| Jane| 24|Germany|\n",
      "| Mary| 24|Germany|\n",
      "| John| 18|  Japan|\n",
      "+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql\n",
    "cust.createOrReplaceTempView(\"custTable\")\n",
    "spark.sql('''select distinct * from custTable''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
